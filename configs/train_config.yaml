data_path: data/concatenated_anekdot_dataset.csv
train_batch_size: 8
val_batch_size: 8
train_size: 0.8
epochs: 200
lr: 5e-5
weight_decay: 1e-4
fp16: True
d_model: 512
nhead: 8
dim_feedforward: 2048
num_layers: 8
vocab_size: 50000
max_len: 300
model_dir: outputs/models
optim: AdamW
device: cuda
patience: 25
inference_size: 4
word_dropout: 0
train_tokenizer: False
tokenizer_path: outputs/tokenizers/tokenizer.json 
seed: 42
dropout: 0.1
latent_dim: 512
tsne: 0
grad_accumulation_steps: 8
warmup_steps: 1


# TRY CYCLIC LR!!!
# Add augmentations: pop random word, synonym augmentation
# Reduce model size (probably bad idea)
# best with lr 5e-3 wd 1e-3  AdamW ed 0.2 dd 0.1 kln 0.1 beta 0.001 ld 256 dm 256 nhd 8  dff 1024 nl 4 vs 50000 ml 300 seed 42
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7dbad73-b930-4c93-b26c-3d42bf561569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8980ed38-c111-4450-b3c2-0913d5e4b85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Брат ебет сестру. Она ему говорит \\n \\n-А ты е...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Идет суд. Судья: \\n— Свидетель Георгадзе! \\nТи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Идет мужик на свидание. В белой рубашечке, кос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Многие мужчины очень романтичные и любят, когд...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Дедушка с бабушкой нашли под кроватью у Колобк...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  Брат ебет сестру. Она ему говорит \\n \\n-А ты е...\n",
       "1  Идет суд. Судья: \\n— Свидетель Георгадзе! \\nТи...\n",
       "2  Идет мужик на свидание. В белой рубашечке, кос...\n",
       "3  Многие мужчины очень романтичные и любят, когд...\n",
       "4  Дедушка с бабушкой нашли под кроватью у Колобк..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_output_vk.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba72744-43fe-4279-8cc8-85caa28517d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'].astype(str).to_csv('train_texts.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e465a43-127b-4b7a-8951-dc64fa5891f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tokenizers) (0.29.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\georg\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde63368-d49e-4151-8829-cf6fd61439a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e828400-bff0-4c6c-8fa6-10252d2dd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()  # Предварительное разбиение по пробелам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0af0d5f-1047-48cc-9257-168aa2edfae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BpeTrainer(\n",
    "    vocab_size=10000,  # Размер словаря\n",
    "    min_frequency=2,  # Игнорировать пары, встречающиеся реже 2 раз\n",
    "    special_tokens=[\"[PAD]\", \"[EOS]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b71518f4-6c36-4d7f-80ca-fc57716eb93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(df['Text'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5e8b2b2-872b-4387-9f46-bde3eaf8f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_id = tokenizer.token_to_id(\"[EOS]\")\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"$A [EOS]\",\n",
    "    special_tokens=[(\"[EOS]\", eos_id)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa554510-0d45-4ffc-b27d-6e3139e78047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenized'] = df['Text'].apply(lambda x: tokenizer.encode(x).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b6100b-2b7d-468f-ac26-945cd4fce64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'length Distribution')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_lengths = [len(row['Tokenized']) for idx, row in df.iterrows()]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(seq_lengths, color='blue', label='Sequence lengths', kde=False, bins=30)\n",
    "plt.xlabel(\"Sequence lengths\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title(\"length Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a971c5d-d7d8-4861-93f6-f786b19e45c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Брат ебет сестру. Она ему говорит \n",
      " \n",
      "-А ты ебешься лучше чем отец. \n",
      " \n",
      "-Я знаю, мне мама говорила.\n",
      "Токенизированные ID: [7023, 4710, 6703, 15, 1320, 739, 728, 14, 173, 708, 9385, 3024, 1615, 903, 1475, 15, 14, 204, 1296, 13, 805, 1727, 6232, 15, 1]\n",
      "Декодированный текст: Брат ебет сестру . Она ему говорит - А ты ебе шься лучше чем отец . - Я знаю , мне мама говорила .\n"
     ]
    }
   ],
   "source": [
    "sample_text = df['Text'].iloc[0]\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(f\"Исходный текст: {sample_text}\")\n",
    "print(f\"Токенизированные ID: {encoded.ids}\")\n",
    "print(f\"Декодированный текст: {tokenizer.decode(encoded.ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea7ba81f-66e0-44d5-8256-44d76d7459a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset when batching\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True) <- add [PAD] tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f36de37a-89fd-47e6-b36c-171042d90f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "558efc0e-caa9-4694-b746-92b34f0e2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "   def __init__(self, d_model, max_len=512):\n",
    "       super(PositionalEncoding, self).__init__()\n",
    "       self.d_model = d_model\n",
    "       self.max_len = max_len\n",
    "\n",
    "       # Create a positional encoding matrix\n",
    "       pe = torch.zeros(max_len, d_model)\n",
    "       position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "       div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "       pe[:, 0::2] = torch.sin(position * div_term)\n",
    "       pe[:, 1::2] = torch.cos(position * div_term)\n",
    "       pe = pe.unsqueeze(0)\n",
    "        \n",
    "       self.register_buffer('pe', pe)\n",
    "\n",
    "   def forward(self, x):\n",
    "       # Add positional embeddings to input token embeddings\n",
    "       # print(f'pe shape = {self.pe[:, :x.size(1), :].size()}')\n",
    "       x = x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "       return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1a6ca10-b75d-4998-9d93-51bc2b0843a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        return self.fc2(self.dropout(out))\n",
    "        \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attention =  nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.feedforward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, cache=None):\n",
    "        if cache is None:\n",
    "            attn_output, _ = self.attention(query=x, key=x, value=x, need_weights=False)\n",
    "            new_cache = {'key': x, 'value': x}\n",
    "        else:\n",
    "            attn_output, _ = self.attention(query=x, key=cache['key'], value=cache['value'], need_weights=False)\n",
    "            new_cache = {\n",
    "                'key': torch.cat([cache['key'], x], dim=1),\n",
    "                'value': torch.cat([cache['value'], x], dim=1)\n",
    "            }\n",
    "        out = self.norm1(x + self.dropout1(attn_output)) # residual connection\n",
    "        feedforward_output = self.feedforward(out)\n",
    "        return self.norm2(out + self.dropout2(feedforward_output)), new_cache\n",
    "\n",
    "\n",
    "class GumbelSoftmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, tau=1.0):\n",
    "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-20))\n",
    "        y = F.softmax((logits + gumbel_noise) / tau, dim=-1)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model=512, num_decoder_layers=6, nhead=8, d_ff=2048, dropout=0.1, maxlen=200, vocab_size=10000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, self.d_model)\n",
    "        self.pos_encoder = PositionalEncoding(self.d_model, maxlen)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(self.d_model, nhead, d_ff, dropout) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(self.d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gumbel_softmax = GumbelSoftmax()\n",
    "\n",
    "    def forward(self, x, max_seq_len=200):\n",
    "        # x.shape = (BATCH_SIZE, 1)\n",
    "        first_iter = True\n",
    "        curr_seq_len = 0\n",
    "        output_seq = x.clone()\n",
    "        generator_seq = self.embedding(x)  # Начальная последовательность эмбеддингов\n",
    "        last_token = x[:, 0]\n",
    "        padding_mask = torch.zeros(x.shape[0], 1, dtype=torch.long).to(device)\n",
    "        eos_in_batch_counts = torch.ones(x.shape[0], dtype=torch.long).to(device)\n",
    "        cache = [None] * len(self.decoder_blocks)\n",
    "        with torch.cuda.amp.autocast():  # Используем FP16\n",
    "            while curr_seq_len < max_seq_len and not torch.all(eos_in_batch_counts >= 2):\n",
    "                out = self.dropout(self.pos_encoder(generator_seq))\n",
    "                for i, decoder_block in enumerate(self.decoder_blocks):\n",
    "                    out, cache[i] = decoder_block(out[:, -1:], cache[i])  # Обрабатываем только последний токен\n",
    "                logits = self.fc(out[:, -1, :])  # Логиты для последнего токена\n",
    "                soft_token = self.gumbel_softmax(logits, tau=0.5)\n",
    "                next_embedding = torch.matmul(soft_token, self.embedding.weight).unsqueeze(1)\n",
    "                generator_seq = torch.cat([generator_seq, next_embedding], dim=1)\n",
    "\n",
    "                tokens = soft_token.argmax(dim=-1)\n",
    "                output_seq = torch.cat([output_seq, tokens.unsqueeze(1)], dim=1)\n",
    "                padding_mask = torch.cat([padding_mask, torch.where(eos_in_batch_counts >= 2, 1, 0).unsqueeze(1)], dim=1)\n",
    "                eos_in_batch_counts[torch.where(tokens == eos_token_id)[0]] += 1\n",
    "                curr_seq_len += 1\n",
    "        \n",
    "        return generator_seq, output_seq, padding_mask\n",
    "\n",
    "    # def forward(self, x, attn_mask=None):\n",
    "    #     out = self.embedding(x)\n",
    "    #     out = self.dropout(self.pos_encoder(out))\n",
    "    #     for decoder_block in self.decoder_blocks:\n",
    "    #         out = decoder_block(out, attn_mask)\n",
    "    #     return out\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfd24513-dcc9-4f4a-862a-71e414004c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d_model=512, num_encoder_layers=6, nhead=8, maxlen=200, vocab_size=10000):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, maxlen)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x, real_data = False, src_key_padding_mask=None):\n",
    "        if real_data:\n",
    "            x = self.embedding(x)\n",
    "        out = self.pos_encoder(x)\n",
    "        out = self.transformer_encoder(out, src_key_padding_mask=src_key_padding_mask)\n",
    "        out = out.mean(dim=1)\n",
    "        return self.fc(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b15844-31f2-4bc7-aa7e-b813e5827d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(src, padding_value=0):\n",
    "    return src == padding_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd0341ed-6dff-4a59-93cd-f4ee4d38ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_mask(x, padding_token):\n",
    "    batch_size, seq_len = x.size()\n",
    "    device = x.device\n",
    "\n",
    "    padding_mask = (x != paddng_token).float()\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    \n",
    "    padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "    combined_mask = torch.where(\n",
    "        (causal_mask == 1) & (padding_mask == 1),\n",
    "        torch.tensor(0.0, device=device),\n",
    "        torch.tensor(float('-inf'), device=device)\n",
    "    )\n",
    "    \n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d662f-76ee-4d4c-971e-a2711e4f2a11",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55263cbc-3bbe-4072-bb6f-4f6067132454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self, df, maxlen=200):\n",
    "        self.maxlen = maxlen\n",
    "        df['Tokenized'] = df['Tokenized'].apply(lambda x: self._pad_sequence(x[:200]))\n",
    "        self.data = df['Tokenized'].to_numpy()\n",
    "    \n",
    "    def _pad_sequence(self, x):\n",
    "        pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "        x += [pad_id] * (self.maxlen - len(x))\n",
    "        return x\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5813782-9643-4307-a80b-cb9923db497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "TRAIN_SIZE = 0.9\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dataset = TextDataset(df)\n",
    "trainset, valset = random_split(dataset, [TRAIN_SIZE, 1 - TRAIN_SIZE])\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2ac01-7753-4371-8683-c6c3a9283153",
   "metadata": {},
   "source": [
    "# Auxiliary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb8c551a-f879-4c31-944a-781d6725b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0 \n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfb0dc-b811-4583-9722-2bf9580c68ab",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d13af63-6ea6-421f-b789-b16c3c822b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27d7bc8c-fcd0-40f2-bab4-b29afbf70223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 8.5.2025 21:9...\n",
      "Warmup steps: 56, training steps: 5650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_22960\\4067392916.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2af727e92f247e680a7a3699a0c7ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_22960\\4067392916.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.14 GiB is allocated by PyTorch, and 566.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# combined_mask = create_combined_mask(noise, padding_value=tokenizer.token_to_id(\"[PAD]\")).to(device)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 66\u001b[0m     generator_out, _, generator_padding_mask \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     disc_pos_out \u001b[38;5;241m=\u001b[39m discriminator(generator_out, real_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, src_key_padding_mask\u001b[38;5;241m=\u001b[39mgenerator_padding_mask)\n\u001b[0;32m     69\u001b[0m     disc_neg_out \u001b[38;5;241m=\u001b[39m discriminator(data, real_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, src_key_padding_mask\u001b[38;5;241m=\u001b[39mdata_padding_mask)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[16], line 73\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x, max_seq_len)\u001b[0m\n\u001b[0;32m     71\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(out))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_blocks:\n\u001b[1;32m---> 73\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]) \u001b[38;5;66;03m# out.shape = (BATCH_SIZE, vocab_size)\u001b[39;00m\n\u001b[0;32m     76\u001b[0m soft_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgumbel_softmax(out, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;66;03m# soft_token.shape = (BATCH_SIZE, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(query\u001b[38;5;241m=\u001b[39mx, key\u001b[38;5;241m=\u001b[39mx, value\u001b[38;5;241m=\u001b[39mx, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn_output)) \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m feedforward_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(feedforward_output))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.14 GiB is allocated by PyTorch, and 566.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "generator = Generator(num_decoder_layers=3, d_ff=1024).to(device)\n",
    "discriminator = Discriminator(num_encoder_layers=3).to(device)\n",
    "\n",
    "NUM_EP = 50\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "generator_optim = Adam(generator.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "discriminator_optim = Adam(discriminator.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "discriminator_criterion = nn.BCELoss()\n",
    "generator_criterion = nn.BCELoss()\n",
    "\n",
    "# Gradient scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience=10, min_delta=0.01)\n",
    "\n",
    "# LR scheduler with warmup\n",
    "num_training_steps = len(trainloader) * NUM_EP\n",
    "num_warmup_steps = int(len(trainloader) * 0.5)\n",
    "generator_scheduler = get_linear_schedule_with_warmup(\n",
    "    generator_optim,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "discriminator_scheduler = get_linear_schedule_with_warmup(\n",
    "    discriminator_optim,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Arrays for saving loss values\n",
    "train_gen_losses = []\n",
    "train_disc_losses = []\n",
    "val_disc_losses = []\n",
    "\n",
    "# Best loss value\n",
    "best_loss = float('inf')\n",
    "\n",
    "training_step = 0\n",
    "\n",
    "current_time = datetime.now()\n",
    "print('Starting at ' + f'{current_time.day}.{current_time.month}.{current_time.year} {current_time.hour}:{current_time.minute}...')\n",
    "print(f'Warmup steps: {num_warmup_steps}, training steps: {num_training_steps}')\n",
    "for epoch in tqdm(range(NUM_EP)):\n",
    "    # Train\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    running_gen_loss = 0\n",
    "    running_disc_loss = 0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        # Padding mask to ignore <pad> (0) tokens\n",
    "        data_padding_mask = create_padding_mask(data, padding_value=tokenizer.token_to_id(\"[PAD]\"))\n",
    "        # Autocasting to fp16\n",
    "        # with autocast():\n",
    "        # Forward pass\n",
    "        # noise = torch.randint(low=0, high=vocab_size, size=(BATCH_SIZE, 1)).to(device)\n",
    "        noise = torch.tensor([tokenizer.token_to_id('[EOS]')] * BATCH_SIZE, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "        # combined_mask = create_combined_mask(noise, padding_value=tokenizer.token_to_id(\"[PAD]\")).to(device)\n",
    "        with autocast():\n",
    "            generator_out, _, generator_padding_mask = generator(noise)\n",
    "        \n",
    "            disc_pos_out = discriminator(generator_out, real_data=False, src_key_padding_mask=generator_padding_mask)\n",
    "            disc_neg_out = discriminator(data, real_data=True, src_key_padding_mask=data_padding_mask)\n",
    "\n",
    "            # Computing loss\n",
    "            disc_pos_loss = discriminator_criterion(disc_pos_out, torch.ones(BATCH_SIZE))\n",
    "            disc_neg_loss = discriminator_criterion(disc_neg_out, torch.zeros(BATCH_SIZE))\n",
    "            disc_loss = disc_pos_loss + disc_neg_loss\n",
    "\n",
    "            gen_loss = generator_criterion(disc_pos_out, torch.zeros(BATCH_SIZE))\n",
    "\n",
    "        # # Backward pass with scaled gradients\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.unscale_(optim)\n",
    "        \n",
    "        # # Gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # # Step of the optimizer\n",
    "        # scaler.step(optim)\n",
    "        # scaler.update()\n",
    "        \n",
    "        discriminator_optim.zero_grad()\n",
    "        scaler.scale(disc_loss).backward()\n",
    "        scaler.unscale_(discriminator_optim)\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "        scaler.step(discriminator_optim)\n",
    "        scaler.update()\n",
    "\n",
    "        generator_optim.zero_grad()\n",
    "        scaler.scale(gen_loss).backward()\n",
    "        scaler.unscale_(generator_optim)\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "        scaler.step(generator_optim)\n",
    "        scaler.update()\n",
    "\n",
    "        running_gen_loss += gen_loss.item()\n",
    "        running_disc_loss += disc_loss.item()\n",
    "\n",
    "        # LR scheduler step\n",
    "        generator_scheduler.step()\n",
    "        discriminator_scheduler.step()\n",
    "\n",
    "        if training_step % 1000 == 0 and training_step < num_warmup_steps:\n",
    "            current_time = datetime.now()\n",
    "            time = f'{current_time.day}.{current_time.month}.{current_time.year} {current_time.hour}:{current_time.minute}'\n",
    "            print(time + f'\\t\\tEpoch: {epoch}, training_step: {training_step}, LR = {generator_scheduler.get_last_lr()[0]}, train gen loss = {gen_loss.item():.3f}, train disc loss = {disc_loss.item():.3f}')\n",
    "        training_step += 1\n",
    "\n",
    "    train_gen_losses.append(running_gen_loss / len(trainloader))\n",
    "    train_disc_losses.append(running_disc_loss / len(trainloader))\n",
    "\n",
    "    # Validation\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    running_disc_val_loss = 0\n",
    "    for data in valloader:\n",
    "        data_padding_mask = create_padding_mask(data, padding_value=tokenizer.token_to_id(\"[PAD]\"))\n",
    "        noise = torch.tensor([tokenizer.token_to_id('[EOS]')] * BATCH_SIZE, dtype=torch.long).unsqueeze(1).to(device)\n",
    "        with autocast():\n",
    "            generator_out, _, generator_padding_mask = generator(noise)\n",
    "            disc_pos_out = discriminator(generator_out, real_data=False, src_key_padding_mask=generator_padding_mask)\n",
    "            disc_neg_out = discriminator(data, real_data=True, src_key_padding_mask=data_padding_mask)\n",
    "            disc_pos_loss = discriminator_criterion(disc_pos_out, torch.ones(BATCH_SIZE))\n",
    "            disc_neg_loss = discriminator_criterion(disc_neg_out, torch.zeros(BATCH_SIZE))\n",
    "            disc_loss = disc_pos_loss + disc_neg_loss\n",
    "\n",
    "        running_disc_val_loss += disc_loss.item()\n",
    "\n",
    "    val_disc_losses.append(running_disc_val_loss / len(valloader))\n",
    "    \n",
    "    # Inferencing\n",
    "    noise = torch.tensor([tokenizer.token_to_id('[EOS]')] * 10, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    with autocast():\n",
    "        _, output_seq, _ = generator(noise)\n",
    "    print(\"Generated texts: \")\n",
    "    for sequence in output_seq:\n",
    "        text = tokenizer.decode(sequence)\n",
    "        print(f'\\t{text}')\n",
    "\n",
    "    current_time = datetime.now()\n",
    "    time = f'{current_time.day}.{current_time.month}.{current_time.year} {current_time.hour}:{current_time.minute}'\n",
    "    print(time + f' Epoch: {epoch}, train gen loss: {train_gen_losses[-1]:.3f}, train disc loss: {train_disc_losses[-1]:.3f}, val disc loss: {val_disc_losses[-1]:.3f}')\n",
    "    \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
